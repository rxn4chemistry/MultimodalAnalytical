metadata:
  task_name: train_pipeline_example
pipeline_phases:
  load_data:
    phase_name: HFDataLoader
    phase_arguments:
      dataset_path: "Salesforce/wikitext"
      dataset_loading_args:
        name: "wikitext-103-raw-v1"
        split: "train"
      dataset_processing_function_string: "dataset.take(500).filter(lambda x: x != '', input_columns=['text'])"
  process_data:
    - phase_name: TokenizerPreprocessor
      phase_arguments:
        tokenizer_prompt: "example['text']"
  modeling:
    tokenizer:
      model_name: HuggingFaceTB/SmolLM-135M
      add_special_tokens:
        pad_token: <PAD>
    collator:
      collator_name: DataCollatorForLanguageModeling
      mlm: false
    model:
      model_name: HuggingFaceTB/SmolLM-135M
      is_pretrained: False
  trainer:
    args:
      output_dir: ""
      num_train_epochs: 1
      evaluation_strategy: epoch
      learning_rate: 0.0003
      per_device_train_batch_size: 4
      per_device_eval_batch_size: 4
      logging_strategy: epoch
      logging_first_step: true
      logging_steps: 1
      save_total_limit: 1
      disable_tqdm: true
      adam_beta1: 0.9
      adam_beta2: 0.95
      lr_scheduler_type: cosine
      gradient_accumulation_steps: 1
      auto_find_batch_size: true
      weight_decay: 0.1
      fp16: false
      max_grad_norm: 1.0
      dataloader_pin_memory: true
      dataloader_num_workers: 1
      report_to: none
      save_strategy: epoch
      save_steps: 1
      metric_for_best_model: eval_loss
      greater_is_better: false
      load_best_model_at_end: true
    train_args:
      resume_from_checkpoint: False
